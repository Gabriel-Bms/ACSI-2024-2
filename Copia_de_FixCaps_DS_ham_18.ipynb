{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8E9GZcOI-uvZ",
        "outputId": "e3f84103-be8a-470e-d05d-fd20c4a1fb13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "Using 4 dataloader workers every process.\n",
            "Using 7 classes.\n",
            "using 48322 images for training, 5970 images for validation, 828 images for testing.\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1          [-1, 3, 141, 141]             975\n",
            "            Conv2d-2          [-1, 128, 71, 71]             512\n",
            "       BatchNorm2d-3          [-1, 128, 71, 71]             256\n",
            "              ReLU-4          [-1, 128, 71, 71]               0\n",
            "FractionalMaxPool2d-5          [-1, 128, 20, 20]               0\n",
            "            Conv2d-6          [-1, 128, 20, 20]          16,384\n",
            "       BatchNorm2d-7          [-1, 128, 20, 20]             256\n",
            "         Hardswish-8          [-1, 128, 20, 20]               0\n",
            " AdaptiveAvgPool2d-9            [-1, 128, 1, 1]               0\n",
            "           Conv2d-10            [-1, 128, 1, 1]          16,384\n",
            "             ReLU-11            [-1, 128, 1, 1]               0\n",
            "           Conv2d-12            [-1, 128, 1, 1]          16,384\n",
            "AdaptiveMaxPool2d-13            [-1, 128, 1, 1]               0\n",
            "           Conv2d-14            [-1, 128, 1, 1]          16,384\n",
            "             ReLU-15            [-1, 128, 1, 1]               0\n",
            "           Conv2d-16            [-1, 128, 1, 1]          16,384\n",
            "          Sigmoid-17            [-1, 128, 1, 1]               0\n",
            " ChannelAttention-18            [-1, 128, 1, 1]               0\n",
            "           Conv2d-19            [-1, 1, 20, 20]              18\n",
            "          Sigmoid-20            [-1, 1, 20, 20]               0\n",
            " SpatialAttention-21            [-1, 1, 20, 20]               0\n",
            "        Conv_CBAM-22          [-1, 128, 20, 20]               0\n",
            "           Conv2d-23             [-1, 16, 6, 6]          10,368\n",
            "         ConvUnit-24             [-1, 16, 6, 6]               0\n",
            "           Conv2d-25             [-1, 16, 6, 6]          10,368\n",
            "         ConvUnit-26             [-1, 16, 6, 6]               0\n",
            "           Conv2d-27             [-1, 16, 6, 6]          10,368\n",
            "         ConvUnit-28             [-1, 16, 6, 6]               0\n",
            "           Conv2d-29             [-1, 16, 6, 6]          10,368\n",
            "         ConvUnit-30             [-1, 16, 6, 6]               0\n",
            "           Conv2d-31             [-1, 16, 6, 6]          10,368\n",
            "         ConvUnit-32             [-1, 16, 6, 6]               0\n",
            "           Conv2d-33             [-1, 16, 6, 6]          10,368\n",
            "         ConvUnit-34             [-1, 16, 6, 6]               0\n",
            "           Conv2d-35             [-1, 16, 6, 6]          10,368\n",
            "         ConvUnit-36             [-1, 16, 6, 6]               0\n",
            "           Conv2d-37             [-1, 16, 6, 6]          10,368\n",
            "         ConvUnit-38             [-1, 16, 6, 6]               0\n",
            "     Primary_Caps-39               [-1, 8, 576]               0\n",
            "      Digits_Caps-40             [-1, 7, 16, 1]               0\n",
            "================================================================\n",
            "Total params: 166,881\n",
            "Trainable params: 166,881\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 1.02\n",
            "Forward/backward pass size (MB): 17.30\n",
            "Params size (MB): 0.64\n",
            "Estimated Total Size (MB): 18.96\n",
            "----------------------------------------------------------------\n",
            "Model | Params(M) | FLOPs(G)\n",
            "---|---|---\n",
            "FixCaps | 0.13 | 0.03\n",
            "suf:0917_140421\n",
            "D:/ACSII_proyecto/FixCaps-main/HAM10000/0917_140421/best_HAM10000_0917_140421.pth\n",
            "\u001b[1;32m[Train Epoch:[1]HAM10000 ==> Training]\u001b[0m ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 189/378 [20:35<19:12,  6.10s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[189/378] Loss0.36791,ACC:0.28799\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 378/378 [40:07<00:00,  5.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[378/378] Loss0.36450,ACC:0.35489\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 378/378 [40:08<00:00,  6.37s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch:[1] Loss:0.40855,Acc:0.35535,Best_train:0.35535\n",
            "\u001b[35mHAM10000 ==> val ...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 193/193 [01:46<00:00,  1.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_Acc:\u001b[1;32m49.129%\u001b[0m\n",
            "Best_val:\u001b[1;32m[49.129%]\u001b[0m\n",
            "\u001b[1;32m[Train Epoch:[2]HAM10000 ==> Training]\u001b[0m ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 189/378 [19:16<19:16,  6.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[189/378] Loss0.36772,ACC:0.44585\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 378/378 [38:36<00:00,  5.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[378/378] Loss0.31728,ACC:0.45974\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 378/378 [38:37<00:00,  6.13s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch:[2] Loss:0.34895,Acc:0.46033,Best_train:0.46033\n",
            "\u001b[35mHAM10000 ==> val ...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 193/193 [01:31<00:00,  2.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_Acc:\u001b[1;32m60.218%\u001b[0m\n",
            "Best_val:\u001b[1;32m[60.218%]\u001b[0m\n",
            "\u001b[1;32m[Train Epoch:[3]HAM10000 ==> Training]\u001b[0m ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 189/378 [19:50<19:37,  6.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[189/378] Loss0.31724,ACC:0.49111\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 378/378 [38:20<00:00,  4.87s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[378/378] Loss0.33644,ACC:0.49733\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 378/378 [38:22<00:00,  6.09s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch:[3] Loss:0.32626,Acc:0.49797,Best_train:0.49797\n",
            "\u001b[35mHAM10000 ==> val ...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 193/193 [01:30<00:00,  2.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_Acc:\u001b[1;32m65.946%\u001b[0m\n",
            "Best_val:\u001b[1;32m[65.946%]\u001b[0m\n",
            "\u001b[1;32m[Train Epoch:[4]HAM10000 ==> Training]\u001b[0m ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 189/378 [18:26<17:46,  5.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[189/378] Loss0.35469,ACC:0.51649\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 378/378 [36:48<00:00,  4.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[378/378] Loss0.31603,ACC:0.52350\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 378/378 [36:50<00:00,  5.85s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch:[4] Loss:0.31225,Acc:0.52417,Best_train:0.52417\n",
            "\u001b[35mHAM10000 ==> val ...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 193/193 [02:15<00:00,  1.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_Acc:\u001b[1;32m68.325%\u001b[0m\n",
            "Best_val:\u001b[1;32m[68.325%]\u001b[0m\n",
            "\u001b[1;32m[Train Epoch:[5]HAM10000 ==> Training]\u001b[0m ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 189/378 [19:31<18:21,  5.83s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[189/378] Loss0.28873,ACC:0.53977\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 378/378 [37:49<00:00,  4.75s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[378/378] Loss0.29799,ACC:0.54076\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 378/378 [37:50<00:00,  6.01s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch:[5] Loss:0.30128,Acc:0.54145,Best_train:0.54145\n",
            "\u001b[35mHAM10000 ==> val ...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 193/193 [01:34<00:00,  2.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_Acc:\u001b[1;32m68.074%\u001b[0m\n",
            "Best_val:\u001b[1;32m[68.325%]\u001b[0m\n",
            "\u001b[1;32m[Train Epoch:[6]HAM10000 ==> Training]\u001b[0m ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 189/378 [18:41<19:23,  6.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[189/378] Loss0.30869,ACC:0.55179\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 378/378 [39:06<00:00,  4.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[378/378] Loss0.28152,ACC:0.54727\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 378/378 [39:07<00:00,  6.21s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch:[6] Loss:0.29805,Acc:0.54797,Best_train:0.54797\n",
            "\u001b[35mHAM10000 ==> val ...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 193/193 [01:37<00:00,  1.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_Acc:\u001b[1;32m68.342%\u001b[0m\n",
            "Best_val:\u001b[1;32m[68.342%]\u001b[0m\n",
            "\u001b[1;32m[Train Epoch:[7]HAM10000 ==> Training]\u001b[0m ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 189/378 [21:26<26:13,  8.33s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[189/378] Loss0.27012,ACC:0.54229\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 378/378 [45:47<00:00,  4.90s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[378/378] Loss0.31724,ACC:0.54861\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 378/378 [45:49<00:00,  7.27s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch:[7] Loss:0.29663,Acc:0.54932,Best_train:0.54932\n",
            "\u001b[35mHAM10000 ==> val ...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 193/193 [01:43<00:00,  1.87it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_Acc:\u001b[1;32m69.615%\u001b[0m\n",
            "Best_val:\u001b[1;32m[69.615%]\u001b[0m\n",
            "\u001b[1;32m[Train Epoch:[8]HAM10000 ==> Training]\u001b[0m ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 189/378 [28:28<1:59:06, 37.81s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[189/378] Loss0.31672,ACC:0.54923\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 378/378 [54:55<00:00,  7.71s/it]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[378/378] Loss0.33027,ACC:0.55068\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 378/378 [54:57<00:00,  8.72s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch:[8] Loss:0.29646,Acc:0.55138,Best_train:0.55138\n",
            "\u001b[35mHAM10000 ==> val ...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 193/193 [01:53<00:00,  1.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_Acc:\u001b[1;32m71.457%\u001b[0m\n",
            "Best_val:\u001b[1;32m[71.457%]\u001b[0m\n",
            "\u001b[1;32m[Train Epoch:[9]HAM10000 ==> Training]\u001b[0m ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 189/378 [27:19<26:40,  8.47s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[189/378] Loss0.28970,ACC:0.55576\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 378/378 [46:33<00:00,  4.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[378/378] Loss0.27276,ACC:0.55975\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 378/378 [46:34<00:00,  7.39s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch:[9] Loss:0.29358,Acc:0.56047,Best_train:0.56047\n",
            "\u001b[35mHAM10000 ==> val ...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 193/193 [01:39<00:00,  1.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_Acc:\u001b[1;32m70.921%\u001b[0m\n",
            "Best_val:\u001b[1;32m[71.457%]\u001b[0m\n",
            "\u001b[1;32m[Train Epoch:[10]HAM10000 ==> Training]\u001b[0m ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 189/378 [20:24<25:53,  8.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[189/378] Loss0.34139,ACC:0.56448\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 378/378 [46:40<00:00,  7.55s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[378/378] Loss0.31465,ACC:0.56802\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 378/378 [46:42<00:00,  7.41s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch:[10] Loss:0.28865,Acc:0.56875,Best_train:0.56875\n",
            "\u001b[35mHAM10000 ==> val ...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 193/193 [01:53<00:00,  1.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_Acc:\u001b[1;32m74.439%\u001b[0m\n",
            "Best_val:\u001b[1;32m[74.439%]\u001b[0m\n",
            "\u001b[1;32m[Train Epoch:[11]HAM10000 ==> Training]\u001b[0m ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 189/378 [27:16<26:58,  8.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[189/378] Loss0.29770,ACC:0.58156\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 378/378 [53:43<00:00,  7.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[378/378] Loss0.30516,ACC:0.58468\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 378/378 [53:45<00:00,  8.53s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch:[11] Loss:0.27998,Acc:0.58543,Best_train:0.58543\n",
            "\u001b[35mHAM10000 ==> val ...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 193/193 [02:02<00:00,  1.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_Acc:\u001b[1;32m72.982%\u001b[0m\n",
            "Best_val:\u001b[1;32m[74.439%]\u001b[0m\n",
            "\u001b[1;32m[Train Epoch:[12]HAM10000 ==> Training]\u001b[0m ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 189/378 [26:55<24:48,  7.88s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[189/378] Loss0.28020,ACC:0.59619\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 378/378 [51:38<00:00,  5.76s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[378/378] Loss0.27183,ACC:0.59981\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 378/378 [51:42<00:00,  8.21s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch:[12] Loss:0.27209,Acc:0.60058,Best_train:0.60058\n",
            "\u001b[35mHAM10000 ==> val ...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 193/193 [01:52<00:00,  1.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_Acc:\u001b[1;32m68.141%\u001b[0m\n",
            "Best_val:\u001b[1;32m[74.439%]\u001b[0m\n",
            "\u001b[1;32m[Train Epoch:[13]HAM10000 ==> Training]\u001b[0m ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 42%|████▏     | 157/378 [17:22<24:27,  6.64s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 310\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;66;03m#base_momentum=0.85, max_momentum=0.95, div_factor=25.0, final_div_factor=10000.0,\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;66;03m# scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=learning_rate,\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;66;03m#                         steps_per_epoch=len(train_loader),\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;66;03m#                         epochs=num_epochs)\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m): \n\u001b[1;32m--> 310\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    311\u001b[0m     test(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinished Training\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "Cell \u001b[1;32mIn[5], line 108\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m    106\u001b[0m output \u001b[38;5;241m=\u001b[39m network(data)\n\u001b[0;32m    107\u001b[0m loss \u001b[38;5;241m=\u001b[39m network\u001b[38;5;241m.\u001b[39mloss(output, target, size_average\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)       \n\u001b[1;32m--> 108\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# scaler.scale(loss).backward()\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m# scaler.step(optimizer)\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# scaler.update()        \u001b[39;00m\n\u001b[0;32m    112\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
            "File \u001b[1;32md:\\ACSII_proyecto\\.conda\\Lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\ACSII_proyecto\\.conda\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\ACSII_proyecto\\.conda\\Lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import sys, os\n",
        "import json\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import prettytable\n",
        "import time\n",
        "sys.setrecursionlimit(15000)\n",
        "from thop.profile import profile\n",
        "\n",
        "from PIL import Image\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchsummary import summary\n",
        "from tqdm import tqdm\n",
        "import seaborn as sns\n",
        "\n",
        "from utils import ImageShow,draw_size_acc,one_hot\n",
        "from utils import confusion_matrix,metrics_scores\n",
        "from model import FixCapsNet\n",
        "\n",
        "def get_data(trans_size='308'):\n",
        "    global test_dataset,train_loader,val_loader,test_loader,train_num,val_num,test_num,n_classes,cla_dict\n",
        "    data_transform = {\n",
        "        \"train\": transforms.Compose([transforms.RandomResizedCrop((299, 299)),\n",
        "                                     transforms.RandomVerticalFlip(),\n",
        "                                     transforms.ToTensor(),\n",
        "                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]),\n",
        "        \"val\": transforms.Compose([transforms.Resize((308, 308)),\n",
        "                                   transforms.CenterCrop((299, 299)),\n",
        "                                   transforms.ToTensor(),\n",
        "                                   transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "                                  ]),\n",
        "        \"test\": transforms.Compose([transforms.Resize((trans_size, trans_size)),\n",
        "                                   transforms.CenterCrop((299, 299)),\n",
        "                                   transforms.ToTensor(),\n",
        "                                   transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "                                  ])\n",
        "        }\n",
        "\n",
        "    data_root = os.path.abspath(os.path.join(os.getcwd()))  # get data root path\n",
        "    # image_path = os.path.join(data_root, \"datasets\",\"HAM10000\")#\n",
        "    # assert os.path.exists(image_path), \"{} path does not exist.\".format(image_path)\n",
        "\n",
        "    train_dataset = datasets.ImageFolder(root='D:/ACSII_proyecto/FixCaps-main/augmentation/train525s8',#direccion de train\n",
        "                                         transform=data_transform[\"train\"])\n",
        "    val_dataset = datasets.ImageFolder(root='D:/ACSII_proyecto/FixCaps-main/augmentation/val525s8',##direccion de val\n",
        "                                            transform=data_transform[\"val\"])\n",
        "    test_dataset = datasets.ImageFolder(root='D:/ACSII_proyecto/FixCaps-main/augmentation/test_dir',#direccion de test\n",
        "                                            transform=data_transform[\"test\"])\n",
        "\n",
        "    train_num = len(train_dataset)\n",
        "    val_num = len(val_dataset)\n",
        "    test_num = len(test_dataset)\n",
        "\n",
        "    data_list = train_dataset.class_to_idx\n",
        "    cla_dict = dict((val, key) for key, val in data_list.items())\n",
        "    n_classes  = len(data_list)\n",
        "    print(f'Using {n_classes } classes.')\n",
        "    # write dict into json file\n",
        "    json_str = json.dumps(cla_dict, indent=4)\n",
        "    with open(f'{img_title}.json', 'w') as json_file:#class_indices\n",
        "        json_file.write(json_str)\n",
        "\n",
        "    pin_memory = True\n",
        "    train_loader = DataLoader(train_dataset,batch_size=BatchSize,\n",
        "                                               pin_memory=pin_memory,\n",
        "                                               shuffle=True,num_workers=nw)\n",
        "    val_loader = DataLoader(val_dataset,batch_size=V_size,\n",
        "                                               pin_memory=pin_memory,\n",
        "                                               shuffle=False,num_workers=nw)\n",
        "    test_loader = DataLoader(test_dataset,batch_size=T_size,\n",
        "                                              pin_memory=pin_memory,\n",
        "                                              shuffle=False,num_workers=nw)\n",
        "\n",
        "    print(\"using {} images for training, {} images for validation, {} images for testing.\".format(train_num,\n",
        "                                                                                                  val_num,\n",
        "                                                                                                  test_num))\n",
        "def train(epoch):\n",
        "    network.train()\n",
        "    global best_train,train_evl_result#,evl_tmp_result\n",
        "    running_loss,r_pre = 0., 0.\n",
        "    print_step = len(train_loader)//2\n",
        "    steps_num = len(train_loader)\n",
        "    tmp_size = BatchSize\n",
        "    print(f'\\033[1;32m[Train Epoch:[{epoch}]{img_title} ==> Training]\\033[0m ...')\n",
        "    optimizer.zero_grad()\n",
        "    train_tmp_result = torch.zeros(n_classes,n_classes)\n",
        "    # scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(tqdm(train_loader)):\n",
        "\n",
        "        batch_idx += 1\n",
        "        target_indices = target\n",
        "        target_one_hot = one_hot(target, length=n_classes)\n",
        "        data, target = Variable(data).to(device), Variable(target_one_hot).to(device)\n",
        "        # data, target =data.to(device), target_one_hot.to(device)\n",
        "        # with torch.cuda.amp.autocast():\n",
        "        output = network(data)\n",
        "        loss = network.loss(output, target, size_average=True)\n",
        "        loss.backward()\n",
        "        # scaler.scale(loss).backward()\n",
        "        # scaler.step(optimizer)\n",
        "        # scaler.update()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        # scheduler.step()#AdamW\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        v_mag = torch.sqrt(torch.sum(output**2, dim=2, keepdim=True))\n",
        "        pred = v_mag.data.max(1, keepdim=True)[1].cpu().squeeze()\n",
        "        r_pre += pred.eq(target_indices.view_as(pred)).squeeze().sum()\n",
        "        tmp_pre = r_pre/(batch_idx*BatchSize)\n",
        "\n",
        "        if batch_idx % print_step == 0:\n",
        "            print(\"[{}/{}] Loss{:.5f},ACC:{:.5f}\".format(batch_idx,len(train_loader),\n",
        "                                                         loss,tmp_pre))\n",
        "        if batch_idx % steps_num == 0 and train_num % tmp_size != 0:\n",
        "            tmp_size = train_num % tmp_size\n",
        "\n",
        "        for i in range(tmp_size):\n",
        "            pred_x = pred.numpy()\n",
        "            train_tmp_result[target_indices[i]][pred_x[i]] +=1\n",
        "\n",
        "        if best_train < tmp_pre and tmp_pre >= 80:\n",
        "            torch.save(network.state_dict(), iter_path)\n",
        "\n",
        "    epoch_acc = r_pre / train_num\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    train_loss_list.append(epoch_loss)\n",
        "    train_acc_list.append(epoch_acc)\n",
        "    scheduler.step()\n",
        "    if best_train < epoch_acc:\n",
        "        best_train = epoch_acc\n",
        "        train_evl_result = train_tmp_result.clone()\n",
        "        torch.save(network.state_dict(), last_path)\n",
        "        torch.save(train_evl_result, f'D:/ACSII_proyecto/FixCaps-main/{img_title}/{suf}/train_evl_result.pth')\n",
        "\n",
        "    print(\"Train Epoch:[{}] Loss:{:.5f},Acc:{:.5f},Best_train:{:.5f}\".format(epoch,epoch_loss,\n",
        "                                                                     epoch_acc,best_train))\n",
        "\n",
        "def test(split=\"test\"):\n",
        "    network.eval()\n",
        "    global test_acc,eval_acc,best_acc,test_evl_result,val_evl_result,evl_tmp_result,net_parameters\n",
        "    cor_loss,correct,Auc, Acc= 0, 0, 0, 0\n",
        "    evl_tmp_result = torch.zeros(n_classes,n_classes)\n",
        "\n",
        "    if split == 'val':\n",
        "        data_loader = val_loader\n",
        "        tmp_size = V_size\n",
        "        data_num = val_num\n",
        "    else:\n",
        "        data_loader = test_loader\n",
        "        tmp_size = T_size\n",
        "        data_num = test_num\n",
        "\n",
        "    steps_num = len(data_loader)\n",
        "    print(f'\\033[35m{img_title} ==> {split} ...\\033[0m')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in enumerate(tqdm(data_loader)):\n",
        "            batch_idx +=1\n",
        "            target_indices = target#torch.Size([batch, 7])\n",
        "            target_one_hot = one_hot(target, length=n_classes)\n",
        "            data, target = Variable(data).to(device), Variable(target_one_hot).to(device)\n",
        "\n",
        "            output= network(data)#torch.Size([batch_size, 7, 16, 1])\n",
        "            v_mag = torch.sqrt(torch.sum(output**2, dim=2, keepdim=True))\n",
        "            pred = v_mag.data.max(1, keepdim=True)[1].cpu()#[9, 2, 1, 1, 6,..., 1, 4, 6, 5, 7,]\n",
        "\n",
        "            if batch_idx % steps_num == 0 and test_num % tmp_size != 0:\n",
        "                tmp_size = data_num % tmp_size\n",
        "\n",
        "            for i in range(tmp_size):\n",
        "                pred_y = pred.numpy()\n",
        "                evl_tmp_result[target_indices[i]][pred_y[i]] +=1\n",
        "\n",
        "    diag_sum = torch.sum(evl_tmp_result.diagonal())\n",
        "    all_sum = torch.sum(evl_tmp_result)\n",
        "    test_acc = 100. * float(torch.div(diag_sum,all_sum))\n",
        "    print(f\"{split}_Acc:\\033[1;32m{round(float(test_acc),3)}%\\033[0m\")\n",
        "\n",
        "    if split == 'val':\n",
        "        val_acc_list.append(test_acc)\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            val_evl_result = evl_tmp_result.clone()#copy.deepcopy(input)\n",
        "            torch.save(network.state_dict(), save_PATH)\n",
        "            torch.save(val_evl_result, f'D:/ACSII_proyecto/FixCaps-main/{img_title}/{suf}/best_evl_result.pth')\n",
        "        print(f\"Best_val:\\033[1;32m[{round(float(best_acc),3)}%]\\033[0m\")\n",
        "    else:\n",
        "        test_acc_list.append(test_acc)\n",
        "        if test_acc > eval_acc:\n",
        "            eval_acc = test_acc\n",
        "            test_evl_result = evl_tmp_result.clone()#copy.deepcopy(input)\n",
        "            torch.save(network.state_dict(), f'D:/ACSII_proyecto/FixCaps-main/{img_title}/{suf}/{split}_best_{img_title}_{suf}.pth')\n",
        "            torch.save(test_evl_result, f'D:/ACSII_proyecto/FixCaps-main/{img_title}/{suf}/{split}_evl_result.pth')\n",
        "        print(f\"Best_eval:\\033[1;32m[{round(float(eval_acc),3)}%]\\033[0m\")\n",
        "if __name__ == '__main__':\n",
        "    sys.path.append(os.pardir)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
        "    img_title = \"HAM10000\"#\"COVID-19\"#\"ISIC2019\"#\"skin_lesion\"#\n",
        "    best_acc = 0.\n",
        "    eval_acc = 0.\n",
        "    best_train = 0.\n",
        "    #defined\n",
        "    try:\n",
        "        print(len(train_acc_list))\n",
        "    except NameError:\n",
        "        train_loss_list = []\n",
        "        train_acc_list = []\n",
        "        test_loss_list = []\n",
        "        test_acc_list = []\n",
        "        test_auc_list = []\n",
        "        val_loss_list = []\n",
        "        val_acc_list = []\n",
        "    #activate ImageShow\n",
        "    show = ImageShow(train_loss_list = train_loss_list,\n",
        "                    train_acc_list = train_acc_list,\n",
        "                    test_loss_list = test_loss_list,\n",
        "                    test_acc_list = test_acc_list,\n",
        "                    test_auc_list = test_auc_list,\n",
        "                    val_loss_list = val_loss_list,\n",
        "                    val_acc_list = val_acc_list,\n",
        "                    )\n",
        "\n",
        "    BatchSize = 128#128#188\n",
        "    V_size = 31\n",
        "    T_size = 31\n",
        "    learning_rate = 0.123\n",
        "    train_doc = \"train525e384png\"\n",
        "    val_doc = \"val525e384png\"\n",
        "    test_doc = \"test525png384\"\n",
        "    nw = min([os.cpu_count(), BatchSize if BatchSize > 1 else 0, 4])\n",
        "    print(f'Using {nw} dataloader workers every process.')\n",
        "    get_data()\n",
        "\n",
        "    # Create capsule network.\n",
        "    n_channels = 3\n",
        "    conv_outputs = 128 #Feature_map\n",
        "    num_primary_units = 8\n",
        "    primary_unit_size = 16 * 6 * 6  # fixme get from conv2d\n",
        "    output_unit_size = 16\n",
        "    img_size = 299\n",
        "    mode='DS'\n",
        "    network = FixCapsNet(conv_inputs=n_channels,\n",
        "                        conv_outputs=conv_outputs,\n",
        "                        primary_units=num_primary_units,\n",
        "                        primary_unit_size=primary_unit_size,\n",
        "                        num_classes=n_classes,\n",
        "                        output_unit_size=16,\n",
        "                        init_weights=True,\n",
        "                        mode=mode)\n",
        "    network = network.to(device)\n",
        "    summary(network,(n_channels,img_size,img_size))\n",
        "\n",
        "\n",
        "    print(\"%s | %s | %s\" % (\"Model\", \"Params(M)\", \"FLOPs(G)\"))\n",
        "    print(\"---|---|---\")\n",
        "    name = \"FixCaps\"\n",
        "    dsize = (1, 3, 299, 299)\n",
        "    inputs = torch.randn(dsize).to(device)\n",
        "    total_ops, total_params = profile(network, (inputs,), verbose=False)\n",
        "    print(\n",
        "        \"%s | %.2f | %.2f\" % (name, total_params / (1000 ** 2), total_ops / (1000 ** 3))\n",
        "        )\n",
        "    #FLOPs(G)--> 0.07(0.08).\n",
        "\n",
        "    network.Convolution\n",
        "\n",
        "\n",
        "\n",
        "    try:\n",
        "        print(f\"suf:{suf}\")\n",
        "    except NameError:\n",
        "        suf = time.strftime(\"%m%d_%H%M%S\", time.localtime())\n",
        "        print(f\"suf:{suf}\")\n",
        "    if os.path.exists(f'D:/ACSII_proyecto/FixCaps-main/{img_title}/{suf}'):\n",
        "        print (f'Store: \"D:/ACSII_proyecto/FixCaps-main/{img_title}/{suf}\"')\n",
        "    else:\n",
        "        os.mkdir(f'D:/ACSII_proyecto/FixCaps-main/{img_title}/{suf}')\n",
        "    iter_path = f'D:/ACSII_proyecto/FixCaps-main/{img_title}/{suf}/train_{img_title}_{suf}.pth'\n",
        "    save_PATH = f'D:/ACSII_proyecto/FixCaps-main/{img_title}/{suf}/best_{img_title}_{suf}.pth'\n",
        "    last_path = f'D:/ACSII_proyecto/FixCaps-main/{img_title}/{suf}/last_{img_title}_{suf}.pth'\n",
        "    print(save_PATH)\n",
        "\n",
        "\n",
        "    num_epochs = 125\n",
        "\n",
        "\n",
        "    # learning_rate = 0.123\n",
        "    def_betas=(0.9, 0.999)\n",
        "    optimizer = optim.Adam(network.parameters(), lr=learning_rate)\n",
        "    # optimizer = optim.AdamW(network.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "    scheduler = lr_scheduler.CosineAnnealingLR(optimizer, 5, eta_min=1e-8, last_epoch=-1)\n",
        "\n",
        "    #base_momentum=0.85, max_momentum=0.95, div_factor=25.0, final_div_factor=10000.0,\n",
        "    # scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=learning_rate,\n",
        "    #                         steps_per_epoch=len(train_loader),\n",
        "    #                         epochs=num_epochs)\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        train(epoch)\n",
        "        test('val')\n",
        "\n",
        "    print('Finished Training')\n",
        "\n",
        "\n",
        "    network.load_state_dict(torch.load(save_PATH))\n",
        "    dict_size ={}\n",
        "    dict_test = {}\n",
        "    for j in range(21,31):\n",
        "        print(f\"size:{j}\")\n",
        "        T_size = j\n",
        "\n",
        "        for i in range(300,325):\n",
        "            get_data(i)\n",
        "            for k in range(5):\n",
        "                test()\n",
        "                if dict_test.get(i) is None or dict_test[i] < test_acc:\n",
        "                    dict_test[i] = test_acc\n",
        "\n",
        "                    if dict_size.get(j) is None or dict_size[j] < test_acc:\n",
        "                        dict_size[j] = test_acc\n",
        "\n",
        "                elif dict_size.get(j) is None or dict_size[j] < test_acc:\n",
        "                        dict_size[j] = test_acc\n",
        "\n",
        "    show.conclusion(img_title=img_title)\n",
        "    sorted(dict_size.items(), key=lambda x: x[1], reverse=True)[0:9]\n",
        "\n",
        "\n",
        "    sorted(dict_test.items(), key=lambda x: x[1], reverse=True)[0:9]\n",
        "\n",
        "\n",
        "    # draw_size_acc(dict_test,custom_path='./tmp',img_title=img_title,suf=suf)\n",
        "\n",
        "    metrics_scores(test_evl_result,n_classes,cla_dict)\n",
        "\n",
        "\n",
        "    # #save\n",
        "    s0 = np.array(train_acc_list)\n",
        "    np.save(f'D:/ACSII_proyecto/FixCaps-main/{img_title}/{suf}/{img_title}_train_acc_{suf}.npy', s0)\n",
        "    s1 = np.array(train_loss_list)\n",
        "    np.save(f'D:/ACSII_proyecto/FixCaps-main/{img_title}/{suf}/{img_title}_train_loss_{suf}.npy', s1)\n",
        "    s2 = np.array(val_acc_list)\n",
        "    np.save(f'D:/ACSII_proyecto/FixCaps-main/{img_title}/{suf}/{img_title}_val_acc_{suf}.npy', s2)\n",
        "    s3 = np.array(test_acc_list)\n",
        "    np.save(f'D:/ACSII_proyecto/FixCaps-main/{img_title}/{suf}/{img_title}_test_acc_{suf}.npy', s2)\n",
        "\n",
        "\n",
        "    torch.cuda.memory.empty_cache()\n",
        "\n",
        "    metrics_scores(test_evl_result,n_classes,cla_dict)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}